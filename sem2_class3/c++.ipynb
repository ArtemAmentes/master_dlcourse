{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### [PYTORCH C++ API](https://pytorch.org/cppdocs/)"
      ],
      "metadata": {
        "id": "WA54HKmAXM22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Installation](https://pytorch.org/cppdocs/installing.html)"
      ],
      "metadata": {
        "id": "tXbKDLXuu0Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!wget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip\n",
        "!unzip libtorch-shared-with-deps-latest.zip"
      ],
      "metadata": {
        "id": "tag1EIGzu28Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS8Y0gVRvJCq",
        "outputId": "97fb20f9-78bd-4a24-8371-db6e5e8bdb01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "libtorch  libtorch-shared-with-deps-latest.zip\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hello world"
      ],
      "metadata": {
        "id": "SM6rPy1Iu3Lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p example-app/build/"
      ],
      "metadata": {
        "id": "2447-abYvcoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/example-app/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE-0PgkN5DH0",
        "outputId": "e70e2ceb-79f2-4294-aa51-b25004441778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/example-app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile example-app.cpp\n",
        "\n",
        "#include <torch/torch.h>\n",
        "#include <iostream>\n",
        "\n",
        "int main() {\n",
        "  torch::Tensor tensor = torch::rand({2, 3});\n",
        "  std::cout << tensor << std::endl;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYlbt8EDu-Eu",
        "outputId": "1a9957c7-67fc-4135-f92b-0a713bca5e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example-app.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CMakeLists.txt\n",
        "\n",
        "cmake_minimum_required(VERSION 3.0 FATAL_ERROR)\n",
        "project(example-app)\n",
        "\n",
        "find_package(Torch REQUIRED)\n",
        "set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\")\n",
        "\n",
        "add_executable(example-app example-app.cpp)\n",
        "target_link_libraries(example-app \"${TORCH_LIBRARIES}\")\n",
        "set_property(TARGET example-app PROPERTY CXX_STANDARD 14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9BsNqekvflf",
        "outputId": "646c513d-46ee-4eb1-acde-2b062964210f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CMakeLists.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/example-app/build\n",
        "!cmake -DCMAKE_PREFIX_PATH=/content/libtorch ..\n",
        "!cmake --build . --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAxBh6nj4LHl",
        "outputId": "0c39352e-0a4c-4ec3-9657-6c449038bcbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/example-app/build\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Torch: /content/libtorch/lib/libtorch.so  \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/example-app/build\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/example-app.dir/example-app.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable example-app\u001b[0m\n",
            "[100%] Built target example-app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./example-app"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J4UCx-bO1bm",
        "outputId": "5ca646cc-4b6b-4dc7-bdcf-2f1ffbafb75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0.5236  0.4927  0.4325\n",
            " 0.4201  0.5260  0.4001\n",
            "[ CPUFloatType{2,3} ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vIjOGB7cgmL",
        "outputId": "c8d5e83c-a30c-49e1-e338-012315610e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ATen"
      ],
      "metadata": {
        "id": "Y5HYtpAaXf30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[at:: namespace docs](https://pytorch.org/cppdocs/api/namespace_at.html#namespace-at)"
      ],
      "metadata": {
        "id": "a-rB1RBZaOHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ATen is fundamentally a tensor library, on top of which almost all other Python and C++ interfaces in PyTorch are built. It provides a core Tensor class, on which many hundreds of operations are defined. Most of these operations have both CPU and GPU implementations, to which the Tensor class will dynamically dispatch based on its type. A small example of using ATen could look as follows:"
      ],
      "metadata": {
        "id": "FTdqs2PRaIG0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElBNW4S1WTK7"
      },
      "outputs": [],
      "source": [
        "#include <ATen/ATen.h>\n",
        "\n",
        "at::Tensor a = at::ones({2, 2}, at::kInt);\n",
        "at::Tensor b = at::randn({2, 2});\n",
        "auto c = a + b.to(at::kInt);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Autograd"
      ],
      "metadata": {
        "id": "a9ooRUada8P1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we term autograd are the portions of PyTorch’s C++ API that augment the ATen Tensor class with capabilities concerning automatic differentiation. The autograd system records operations on tensors to form an autograd graph. Calling backwards() on a leaf variable in this graph performs reverse mode differentiation through the network of functions and tensors spanning the autograd graph, ultimately yielding gradients. The following example provides a taste of this interface:"
      ],
      "metadata": {
        "id": "2iN8_FINbBOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#include <torch/csrc/autograd/variable.h>\n",
        "#include <torch/csrc/autograd/function.h>\n",
        "\n",
        "torch::Tensor a = torch::ones({2, 2}, torch::requires_grad());\n",
        "torch::Tensor b = torch::randn({2, 2});\n",
        "auto c = a + b;\n",
        "c.backward(); // a.grad() will now hold the gradient of c w.r.t. a."
      ],
      "metadata": {
        "id": "7h2cr8vibBc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The at::Tensor class in ATen is not differentiable by default. To add the differentiability of tensors the autograd API provides, you must use tensor factory functions from the torch:: namespace instead of the at:: namespace. For example, while a tensor created with at::ones will not be differentiable, a tensor created with torch::ones will be.**"
      ],
      "metadata": {
        "id": "aUK8REQ1bYD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C++ Frontend"
      ],
      "metadata": {
        "id": "w1jTTp10bZ3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Docs](https://pytorch.org/cppdocs/frontend.html)"
      ],
      "metadata": {
        "id": "5Iz8umfocC9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PyTorch C++ frontend provides a high level, pure C++ modeling interface for neural network and general ML(Machine Learning) research and production use cases, largely following the Python API in design and provided functionality."
      ],
      "metadata": {
        "id": "34XGLiwubeHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Unless you have a particular reason to constrain yourself exclusively to ATen or the Autograd API, the C++ frontend is the recommended entry point to the PyTorch C++ ecosystem."
      ],
      "metadata": {
        "id": "8ivQSChVcv-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Examples for autograd and c++ frontend"
      ],
      "metadata": {
        "id": "ZHAz_523mxDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pytorch/examples/tree/main/cpp/autograd"
      ],
      "metadata": {
        "id": "iE9EHTzUm4ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/notes/extending.html#example"
      ],
      "metadata": {
        "id": "e1rSwocP4pmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TorchScript"
      ],
      "metadata": {
        "id": "6LhAjZRHcGmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TorchScript is an intermediate representation of a PyTorch model that can then be run in a high-performance environment such as C++."
      ],
      "metadata": {
        "id": "7bGm53SBc0It"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html\n",
        "\n",
        "https://pytorch.org/tutorials/advanced/cpp_export.html"
      ],
      "metadata": {
        "id": "u2aLCflUmi1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html\n",
        "\n",
        "https://github.com/pytorch/extension-script/tree/master/example_app"
      ],
      "metadata": {
        "id": "-rPPJ8aYo8zO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C++ extensions"
      ],
      "metadata": {
        "id": "iGrBJV3Peu61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C++ Extensions offer a simple yet powerful way of accessing all of the above interfaces for the purpose of extending regular Python use-cases of PyTorch. C++ extensions are most commonly used to implement custom operators in C++ or CUDA to accelerate research in vanilla PyTorch setups. The C++ extension API does not add any new functionality to the PyTorch C++ API. Instead, it provides integration with Python setuptools as well as JIT compilation mechanisms that allow access to ATen, the autograd and other C++ APIs from Python"
      ],
      "metadata": {
        "id": "7HXmX8Tyfr3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/notes/extending.html#example - Python custom operators\n",
        "https://github.com/pytorch/examples/blob/main/cpp/autograd/autograd.cpp - C++ custom operators"
      ],
      "metadata": {
        "id": "bHWzNX82lZlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p custom_op/build/"
      ],
      "metadata": {
        "id": "3UFtFpN6uNQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/custom_op"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnSNKV75uYhO",
        "outputId": "530c0cbf-2e64-4199-d682-d18a4c2b66eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/custom_op\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mulconstant.cpp\n",
        "\n",
        "#include <torch/torch.h>\n",
        "#include <iostream>\n",
        "\n",
        "using namespace torch::autograd;\n",
        "\n",
        "class MulConstant : public Function<MulConstant> {\n",
        " public:\n",
        "  static torch::Tensor forward(AutogradContext *ctx, torch::Tensor tensor, double constant) {\n",
        "    // ctx is a context object that can be used to stash information\n",
        "    // for backward computation\n",
        "    ctx->saved_data[\"constant\"] = constant;\n",
        "    return tensor * constant;\n",
        "  }\n",
        "\n",
        "  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n",
        "    // We return as many input gradients as there were arguments.\n",
        "    // Gradients of non-tensor arguments to forward must be `torch::Tensor()`.\n",
        "    return {grad_outputs[0] * ctx->saved_data[\"constant\"].toDouble(), torch::Tensor()};\n",
        "  }\n",
        "};\n",
        "\n",
        "void custom_autograd_function_example() {\n",
        "  std::cout << \"====== Running \\\"Using custom autograd function in C++\\\" ======\" << std::endl;\n",
        "  {\n",
        "    auto x = torch::randn({2}).requires_grad_();\n",
        "    auto y = MulConstant::apply(x, 5.5);\n",
        "    y.sum().backward();\n",
        "\n",
        "    std::cout << \"Градиент:\" << x.grad() << std::endl;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  custom_autograd_function_example();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pFxI45IucTH",
        "outputId": "7096af97-d231-4536-aeda-4f29d2263078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mulconstant.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CMakeLists.txt\n",
        "\n",
        "cmake_minimum_required(VERSION 2.8)\n",
        "\n",
        "project(custom_op)\n",
        "set(CMAKE_CXX_STANDARD 14)\n",
        "\n",
        "find_package(Torch REQUIRED)\n",
        "\n",
        "add_executable(${PROJECT_NAME} \"mulconstant.cpp\")\n",
        "target_link_libraries(${PROJECT_NAME} \"${TORCH_LIBRARIES}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQsFmuvjvJ0S",
        "outputId": "101f0046-362c-40c1-8960-ad6906036a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CMakeLists.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRF7mZkQwkp8",
        "outputId": "e06115fc-bdcc-4f93-c956-1318c963cdf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/custom_op/build\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake -DCMAKE_PREFIX_PATH=/content/libtorch ..\n",
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqItpaSvwoLL",
        "outputId": "51ecff2d-822d-4f51-f6ca-2a75736a9cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 2.8.12 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Torch: /content/libtorch/lib/libtorch.so  \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/custom_op/build\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/custom_op.dir/mulconstant.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable custom_op\u001b[0m\n",
            "[100%] Built target custom_op\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./custom_op"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdNkidilw0oD",
        "outputId": "f711fb95-936c-4839-9585-64229cfe08e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====== Running \"Using custom autograd function in C++\" ======\n",
            "Градиент: 5.5000\n",
            " 5.5000\n",
            "[ CPUFloatType{2} ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtvN0OCD3qGT",
        "outputId": "57a62946-f3f1-4ad8-b044-458a7183c624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced example"
      ],
      "metadata": {
        "id": "hdZym7LjdCPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This recurrent unit is similar to an LSTM, but differs in that it lacks a forget gate and uses an Exponential Linear Unit (ELU) as its internal activation function. Because this unit never forgets, we’ll call it LLTM, or Long-Long-Term-Memory unit."
      ],
      "metadata": {
        "id": "lgsvzq1pKvPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.randn(batch_size, input_features)\n",
        "h = torch.randn(batch_size, state_size)\n",
        "C = torch.randn(batch_size, state_size)\n",
        "\n",
        "rnn = LLTM(input_features, state_size)\n",
        "\n",
        "new_h, new_C = rnn(X, (h, C))"
      ],
      "metadata": {
        "id": "g7C-j0QsKrDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/tutorials/advanced/cpp_extension.html?highlight=c\n",
        "https://github.com/pytorch/tutorials/blob/master/advanced_source/cpp_extension.rst?ysclid=ldexhqvmhe807255845  \n",
        "https://github.com/pytorch/extension-cpp"
      ],
      "metadata": {
        "id": "OwU5gGVJUTPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Домашнее задание:"
      ],
      "metadata": {
        "id": "jAs2rJpKX-HZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализовать выбранный оператор на Python и с++. Любым их рассмотренных способов провести запуск Python кода в c++ Frontend и с++ кода в .py"
      ],
      "metadata": {
        "id": "WxrSf3oFYDG7"
      }
    }
  ]
}